{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf74052",
   "metadata": {},
   "source": [
    "## Split and split stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1189520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION: angry\n",
      "EMOTION: neutral\n",
      "EMOTION: bored\n",
      "EMOTION: happy\n",
      "EMOTION: disgust\n",
      "EMOTION: anxious\n",
      "EMOTION: sad\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    n_splits = 5\n",
    "    dataset_location = 'EmoDB/'\n",
    "    output_location = 'MESD_Final_Splits/'\n",
    "    os.makedirs(output_location, exist_ok=True)\n",
    "    emotion_counter = {}\n",
    "    emotion_to_audio = {}\n",
    "    emotions = {\n",
    "        'W': 'angry',\n",
    "        'L': 'bored',\n",
    "        'E': 'disgust',\n",
    "        'A': 'anxious',\n",
    "        'F': 'happy',\n",
    "        'T': 'sad',\n",
    "        'N': 'neutral'\n",
    "    }\n",
    "\n",
    "\n",
    "    for audiofile in os.listdir(dataset_location):\n",
    "        if audiofile == 'desktop.ini':\n",
    "            continue\n",
    "        emotion = emotions[audiofile[5]] # get emotion for that file\n",
    "        \n",
    "        #Emotion counter\n",
    "        if emotion not in emotion_counter:\n",
    "            emotion_counter[emotion] = 0\n",
    "        emotion_counter[emotion] += 1\n",
    "\n",
    "        #Add files in emotion\n",
    "        if emotion not in emotion_to_audio:\n",
    "            emotion_to_audio[emotion] = []\n",
    "        emotion_to_audio[emotion].append(audiofile)\n",
    "        \n",
    "    #Random shuffle the files\n",
    "    for emotion in emotion_to_audio:\n",
    "        random.shuffle(emotion_to_audio[emotion])\n",
    "        random.shuffle(emotion_to_audio[emotion])\n",
    "\n",
    "    #Make 5 splits\n",
    "    for emotion in emotion_to_audio:\n",
    "        print('EMOTION:',emotion)\n",
    "        for i in range(n_splits):\n",
    "            split_location = output_location + 'split_' + str(i) + '/'\n",
    "            os.makedirs(split_location, exist_ok=True)\n",
    "        \n",
    "            max_len = len(emotion_to_audio[emotion])\n",
    "            split_size = max_len // n_splits\n",
    "\n",
    "            start_range = i * split_size\n",
    "            end_range =  (i+1) * split_size\n",
    "\n",
    "            for filename in emotion_to_audio[emotion][start_range : end_range]:\n",
    "                source_location = dataset_location + filename\n",
    "                target_location = split_location + filename\n",
    "                shutil.copyfile(source_location, target_location)\n",
    "\n",
    "        #randomly place remaining files in different splits\n",
    "        for filename in emotion_to_audio[emotion][end_range : max_len]:\n",
    "            selected_split = random.sample([0,1,2,3,4], 1)[0]\n",
    "            source_location = dataset_location + filename\n",
    "            target_location = output_location + 'split_' + str(selected_split) + '/' + filename\n",
    "            shutil.copyfile(source_location, target_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec42245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT: 0\n",
      "-- angry : 87\n",
      "-- neutral : 50\n",
      "-- disgust : 29\n",
      "-- anxious : 47\n",
      "-- bored : 55\n",
      "-- happy : 46\n",
      "-- sad : 43\n",
      "TOTAL: 357\n",
      "------------------------------\n",
      "SPLIT: 1\n",
      "-- angry : 81\n",
      "-- neutral : 52\n",
      "-- bored : 58\n",
      "-- happy : 43\n",
      "-- anxious : 48\n",
      "-- disgust : 31\n",
      "-- sad : 39\n",
      "TOTAL: 352\n",
      "------------------------------\n",
      "SPLIT: 2\n",
      "-- neutral : 50\n",
      "-- bored : 52\n",
      "-- happy : 44\n",
      "-- angry : 85\n",
      "-- disgust : 31\n",
      "-- anxious : 44\n",
      "-- sad : 40\n",
      "TOTAL: 346\n",
      "------------------------------\n",
      "SPLIT: 3\n",
      "-- neutral : 55\n",
      "-- happy : 46\n",
      "-- angry : 84\n",
      "-- disgust : 29\n",
      "-- bored : 50\n",
      "-- sad : 42\n",
      "-- anxious : 46\n",
      "TOTAL: 352\n",
      "------------------------------\n",
      "SPLIT: 4\n",
      "-- angry : 84\n",
      "-- neutral : 51\n",
      "-- disgust : 32\n",
      "-- anxious : 49\n",
      "-- happy : 46\n",
      "-- sad : 41\n",
      "-- bored : 51\n",
      "TOTAL: 354\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_location = 'MESD_Final_Splits/'\n",
    "\n",
    "    for split in range(5):\n",
    "        print('SPLIT:', split)\n",
    "        split_location = dataset_location + 'split_' + str(split)\n",
    "        emotion_counter = {}\n",
    "\n",
    "        for audiofile in os.listdir(split_location):\n",
    "            emotion = emotions[audiofile[5]]\n",
    "        \n",
    "            #Emotion counter\n",
    "            if emotion not in emotion_counter:\n",
    "                emotion_counter[emotion] = 0\n",
    "            emotion_counter[emotion] += 1\n",
    "\n",
    "\n",
    "        for emotion in emotion_counter:\n",
    "            print('--', emotion, ':', emotion_counter[emotion])\n",
    "\n",
    "        print('TOTAL:', sum(emotion_counter.values()))\n",
    "\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ef1bf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927ec740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self, model_name, label_size=7):\n",
    "        super().__init__()\n",
    "        #Input Dim = Batch * 12 * Seq_Len * 768\n",
    "        \n",
    "        if model_name.find('BASE'):\n",
    "            num_layers = 12\n",
    "            feature_dim = 768\n",
    "        elif model_name.find('LARGE'):\n",
    "            num_layers = 24\n",
    "            feature_dim = 1024\n",
    "           \n",
    "        hidden_dim = 256\n",
    "\n",
    "        #Averaging over 12 layers \n",
    "        self.aggr = nn.Conv1d(in_channels=num_layers, out_channels=1, kernel_size=1, bias=False)\n",
    "        \n",
    "        #Input Dim = Batch * Seq_Len * 768\n",
    "        self.cnn  = nn.Conv1d(in_channels=feature_dim, out_channels=hidden_dim, kernel_size=1)\n",
    "        self.cnn2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(0.2)#not used yet\n",
    "\n",
    "        self.linear = nn.Linear(in_features = hidden_dim, out_features = label_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths, device):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, n_layers, seq_len, n_features = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        \n",
    "        #Take average of 12 layers\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        x = self.aggr(x)\n",
    "        x = torch.reshape(x, (batch_size, seq_len, n_features))\n",
    "\n",
    "        #Pass through CNN\n",
    "        x = x.transpose(1,2) #now dimension is batch * n_features * seq_len\n",
    "        x = F.relu(self.cnn(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = x.transpose(1,2) #now dimension is batch * seq_len * n_features\n",
    "\n",
    "        #Do global average over time sequence\n",
    "        global_avg = torch.tensor([]).to(device)\n",
    "        for i in range(batch_size):\n",
    "            mean_vector = torch.mean(x[i,:lengths[i],:], dim = 0)\n",
    "            mean_vector = mean_vector.reshape(1,-1)\n",
    "            global_avg = torch.cat((global_avg, mean_vector))\n",
    "\n",
    "        logits = self.linear(global_avg)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ICASSP3CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dims = 12, embed_size=128, hidden_size=512, num_lstm_layers = 2, bidirectional = False, label_size=7):\n",
    "        super().__init__()\n",
    "        self.n_layers = num_lstm_layers \n",
    "        self.hidden = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.aggr = nn.Conv1d(in_channels=dims, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.embed = nn.Linear(in_features = vocab_size, out_features = embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(3 * embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 3 * embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_lstm_layers, \n",
    "                            bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(in_features = 2 * hidden_size if bidirectional else hidden_size, \n",
    "                                out_features = label_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "        n, d, b, t = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        input = self.aggr(x)\n",
    "        input = torch.reshape(input, (n, b, t))\n",
    "        input = self.embed(input)\n",
    "\n",
    "        batch_size = input.size(0)\n",
    "        input = input.transpose(1,2)    # (B,T,H) -> (B,H,T)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_n = hn.view(self.n_layers, 2, batch_size, self.hidden)\n",
    "            h_n = torch.cat([ h_n[-1, 0,:], h_n[-1,1,:] ], dim = 1)\n",
    "        else:\n",
    "            h_n = hn[-1]\n",
    "\n",
    "        logits = self.linear(h_n)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d837f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "\n",
    "class german_dataset(Dataset):\n",
    "    def __init__(self, dataset_location, data_splits, model_name):\n",
    "        self.dataset_location = dataset_location\n",
    "        self.data_splits = data_splits\n",
    "\n",
    "        #initialize label mapping \n",
    "        self.emotion_mapper = {\n",
    "                            'W': 'angry',\n",
    "                            'L': 'bored',\n",
    "                            'E': 'disgust',\n",
    "                            'A': 'anxious',\n",
    "                            'F': 'happy',\n",
    "                            'T': 'sad',\n",
    "                            'N': 'neutral'}\n",
    "        self.emotion_to_label = {\n",
    "                            'neutral':0,\n",
    "                            'angry':1,\n",
    "                            'anxious':2,\n",
    "                            'bored':3,\n",
    "                            'disgust':4,\n",
    "                            'happy':5,\n",
    "                            'sad':6}\n",
    "        self.label_to_emotion = {\n",
    "                            0:'neutral',\n",
    "                            1:'angry',\n",
    "                            2:'anxious',\n",
    "                            3:'bored',\n",
    "                            4:'disgust',\n",
    "                            5:'happy',\n",
    "                            6:'sad'}\n",
    "\n",
    "        #get all audiofile locations \n",
    "        self._get_data_locations()\n",
    "\n",
    "        #load model\n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if model_name == 'WAV2VEC2_BASE':\n",
    "            self.bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "        elif model_name == 'WAV2VEC2_LARGE':\n",
    "            self.bundle = torchaudio.pipelines.WAV2VEC2_LARGE\n",
    "        elif model_name == 'WAV2VEC2_BASE_XLSR':\n",
    "            self.bundle = torchaudio.pipelines.WAV2VEC2_XLSR53\n",
    "        elif model_name == 'WAV2VEC2_LARGE_XLSR':\n",
    "            self.bundle = torchaudio.pipelines.WAV2VEC2_XLSR_300M\n",
    "        elif model_name == 'HUBERT_BASE':\n",
    "            self.bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "        elif model_name == 'HUBERT_LARGE':\n",
    "            self.bundle = torchaudio.pipelines.HUBERT_LARGE\n",
    "        elif model_name == 'WAVLM_BASE':\n",
    "            self.bundle = torchaudio.pipelines.WAVLM_BASE\n",
    "        elif model_name == 'WAVLM_LARGE':\n",
    "            self.bundle = torchaudio.pipelines.WAVLM_LARGE\n",
    "\n",
    "        self.model = self.bundle.get_model().to(self.device)\n",
    "\n",
    "    def _get_data_locations(self):\n",
    "        #gatherting all audio file locations\n",
    "        self.all_data = []\n",
    "        for split in self.data_splits:\n",
    "            split_location = self.dataset_location + 'split_' + str(split) + '/'\n",
    "            for audiofile in os.listdir(split_location):\n",
    "                filename = split_location + audiofile \n",
    "                self.all_data.append(filename)\n",
    "\n",
    "        random.shuffle(self.all_data)\n",
    "        self.len = len(self.all_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "         \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        audiofile = self.all_data[index]\n",
    "        emotion_id = audiofile.split('/')[-1][5]\n",
    "        emotion = self.emotion_mapper[emotion_id]\n",
    "        emotion_label = self.emotion_to_label[emotion]\n",
    "\n",
    "        # step 2: load audio and features\n",
    "        wave, sr = torchaudio.load(audiofile)\n",
    "        wave = wave.to(self.device)\n",
    "        if sr != self.bundle.sample_rate:\n",
    "            wave = torchaudio.functional.resample(wave, sr, self.bundle.sample_rate)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            features, _ = self.model.extract_features(wave)\n",
    "\n",
    "        #concatenate all features for the 12 layers\n",
    "        features_pt = torch.tensor([])\n",
    "        for layer in range(len(features)):\n",
    "            features_pt = torch.cat((features_pt, features[layer].detach().cpu()), dim = 0)\n",
    "        seq_length = features_pt.shape[1]\n",
    "\n",
    "        return features_pt, emotion_label, seq_length\n",
    "\n",
    "\n",
    "def my_collate_function(data):\n",
    "\n",
    "    features, labels, seq_lengths = zip(*data)\n",
    "    batch_size = len(features)\n",
    "    max_seq_sen = max(seq_lengths)\n",
    "\n",
    "    #FEATURES HAS DIMENSIONS 12 * Seq_Len * feature_dim\n",
    "    features_collated = torch.zeros((batch_size, features[0].shape[0], max_seq_sen, features[0].shape[2]))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        features_collated[i,:,:seq_lengths[i], :] = features[i]\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    seq_lengths = torch.tensor(seq_lengths)\n",
    "\n",
    "    return features_collated, labels, seq_lengths\n",
    "\n",
    "\n",
    "\n",
    "def initialize_data(dataset_location, train_splits, test_splits, model_name):\n",
    "    training_set = german_dataset(dataset_location, train_splits, model_name)\n",
    "    testing_set = german_dataset(dataset_location, test_splits, model_name)\n",
    "\n",
    "    train_params = {'batch_size': 32,\n",
    "              'shuffle': True,\n",
    "              }\n",
    "\n",
    "    test_params = {'batch_size': 32,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 4\n",
    "              }\n",
    "\n",
    "    train_loader = DataLoader(training_set, **train_params, collate_fn = my_collate_function)\n",
    "    test_loader = DataLoader(testing_set, **test_params, collate_fn = my_collate_function)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e59339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 1\n",
      "Train Accuracy : 0.4472934472934473\n",
      "Test Accuracy : 0.6862745098039216\n",
      "Time: 4.780958616733551\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "Train Accuracy : 0.8155270655270656\n",
      "Test Accuracy : 0.9215686274509803\n",
      "Time: 4.7107259432474775\n",
      "------------------------------\n",
      "Epoch: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "def train_model(my_dataloader, model, criterion, optimizer, train_flag = True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if train_flag:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for i, (data, labels, lengths) in enumerate(my_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #send data to device\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #train model\n",
    "        logits = model(data, lengths, device)\n",
    "        if train_flag:\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #store predictions\n",
    "        predictions = torch.argmax(logits, dim = 1).detach().cpu().tolist()\n",
    "        labels = labels.detach().cpu().tolist()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print('Train' if train_flag else 'Test', 'Accuracy :', accuracy)\n",
    "\n",
    "    return model, accuracy\n",
    "\n",
    "\n",
    "for model_index in range(4):\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"List fish in aquarium.\")\n",
    "#     parser.add_argument(\"--model\", type=str, help='WAV2VEC2_BASE/WAV2VEC2_LARGE/HUBERT_BASE/HUBERT_LARGE')\n",
    "#     # args = parser.parse_args()\n",
    "#     # print(1)\n",
    "    args = 'WAV2VEC2_BASE/WAV2VEC2_LARGE/HUBERT_BASE/HUBERT_LARGE'.split('/')\n",
    "    dataset_location = 'MESD_Final_Splits/'\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    logfilename = 'training_logs_' + args[model_index] + '.txt'\n",
    "    f = open(logfilename, 'w')\n",
    "    f.close()\n",
    "\n",
    "    all_split_accuracies = []\n",
    "    for test_split in range(5):\n",
    "        #initialize model\n",
    "        model = Dense(args[model_index])\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        #initialize dataset\n",
    "        train_splits = [0,1,2,3,4]\n",
    "        train_splits.remove(test_split)\n",
    "        train_loader, test_loader = initialize_data(dataset_location, train_splits, [test_split],args[model_index])\n",
    "\n",
    "        best_test_accuracy = 0\n",
    "        for epoch in range(20):\n",
    "\n",
    "            #training\n",
    "            start_time = time.time()\n",
    "            print('Epoch:', epoch + 1)\n",
    "\n",
    "            model, _ = train_model(train_loader, model, criterion, optimizer, True)\n",
    "            _, test_accuracy = train_model(test_loader, model, criterion, optimizer, False)\n",
    "\n",
    "            print('Time:', (time.time() - start_time) / 60)\n",
    "            print('-'*30)\n",
    "\n",
    "            #save model\n",
    "            if test_accuracy > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy\n",
    "                model_save_location = 'Models/' + args[model_index] + '/'\n",
    "                os.makedirs(model_save_location, exist_ok=True)\n",
    "                model_save_path = model_save_location + 'split_' + str(test_split) + '.pt'\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                agg_weights = ' '.join([str(weight[0]) for weight in model.aggr.state_dict()['weight'][0].detach().cpu().tolist()])\n",
    "\n",
    "        \n",
    "        all_split_accuracies.append(best_test_accuracy)\n",
    "        f = open(logfilename, 'a')\n",
    "        f.write('SPLIT : ' + str(test_split) + '|' + str(best_test_accuracy) + '|' + agg_weights + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    mean_accuracy = np.mean(np.array(all_split_accuracies))\n",
    "    std_accuracy = np.std(np.array(all_split_accuracies))\n",
    "\n",
    "    f = open(logfilename, 'a')\n",
    "    f.write('MEAN : ' + str(mean_accuracy) + '| STD:' + str(std_accuracy) + '\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ae8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb06d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a88638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
