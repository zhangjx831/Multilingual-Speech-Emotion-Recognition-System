{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf63137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_paths, file_names, emotions, audios = [], [], [], []\n",
    "emotion_map = {'anger': 'angry', 'happiness': 'happy', 'sadness': 'sad', 'fear': 'fear',\n",
    "              'disgust': 'disgust'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2348eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "0.12.1+cu113\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52db6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_LARGE\n",
    "extractor = bundle.get_model()\n",
    "print(extractor.__class__)\n",
    "print(bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeab751f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'session_entries.csv',\n",
       " 'fear',\n",
       " 'Tools and Documentation',\n",
       " 'models',\n",
       " 'disgust',\n",
       " 'sadness',\n",
       " 'happiness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('../emotiondata/emotion_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6af5a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "folder_list = ['anger', 'disgust', 'fear', 'happiness', 'sadness']\n",
    "entries = []\n",
    "for folder in folder_list:\n",
    "    cur_file_list = os.listdir(f'../emotiondata/emotion_data/{folder}')\n",
    "    for i in cur_file_list:\n",
    "        if i == 's05 (3).wav':\n",
    "            print(\"found\")\n",
    "            continue\n",
    "        entries.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1af534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(entries)\n",
    "session = []\n",
    "equal_parts = (len(entries)-1)//5 # for equally split the entries into 5 parts\n",
    "count = 0\n",
    "main_data_path = '../emotiondata/emotion_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a10f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 604/604 [00:00<00:00, 368971.69it/s]\n"
     ]
    }
   ],
   "source": [
    "### Only Run once\n",
    "from tqdm import tqdm \n",
    "folder_map = {'a':'anger', 'd':'disgust', 'f':'fear', 'h':'happiness', 's':'sadness'}\n",
    "\n",
    "file_paths = []\n",
    "file_names = []\n",
    "emotions = []\n",
    "# audios = []\n",
    "# labels = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(entries))):\n",
    "    entry = entries[i]\n",
    "    if \"wav\" not in entry:\n",
    "        continue\n",
    "    folder = folder_map[entry[0]]\n",
    "    file_path = f'../emotiondata/emotion_data/{folder}/{entry}'\n",
    "    emotion = emotion_map[folder]\n",
    "    file_paths.append(file_path)\n",
    "    file_names.append(entry)\n",
    "    emotions.append(emotion)\n",
    "\n",
    "    # assign session to it\n",
    "    part = (count//equal_parts)%6 + 1\n",
    "    if part == 6:\n",
    "        part = 5\n",
    "    session.append(part)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "file = pd.DataFrame({'path':file_paths, 'name': file_names, 'emotion': emotions, 'session': session})\n",
    "dataframe_path = main_data_path + '/session_entries.csv'\n",
    "file.to_csv(dataframe_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfe9b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'session_entries.csv',\n",
       " 'fear',\n",
       " 'Tools and Documentation',\n",
       " 'models',\n",
       " 'disgust',\n",
       " 'sadness',\n",
       " 'happiness']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../emotiondata/emotion_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcd016",
   "metadata": {},
   "source": [
    "# Extract Features using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791551f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>name</th>\n",
       "      <th>emotion</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../emotiondata/emotion_data/anger/a17 (6).wav</td>\n",
       "      <td>a17 (6).wav</td>\n",
       "      <td>angry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../emotiondata/emotion_data/fear/f02 (3).wav</td>\n",
       "      <td>f02 (3).wav</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../emotiondata/emotion_data/happiness/h06 (2).wav</td>\n",
       "      <td>h06 (2).wav</td>\n",
       "      <td>happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../emotiondata/emotion_data/fear/f02 (2).wav</td>\n",
       "      <td>f02 (2).wav</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../emotiondata/emotion_data/happiness/h13 (5).wav</td>\n",
       "      <td>h13 (5).wav</td>\n",
       "      <td>happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path         name emotion  \\\n",
       "0      ../emotiondata/emotion_data/anger/a17 (6).wav  a17 (6).wav   angry   \n",
       "1       ../emotiondata/emotion_data/fear/f02 (3).wav  f02 (3).wav    fear   \n",
       "2  ../emotiondata/emotion_data/happiness/h06 (2).wav  h06 (2).wav   happy   \n",
       "3       ../emotiondata/emotion_data/fear/f02 (2).wav  f02 (2).wav    fear   \n",
       "4  ../emotiondata/emotion_data/happiness/h13 (5).wav  h13 (5).wav   happy   \n",
       "\n",
       "   session  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "main_data_path = '../emotiondata/emotion_data'\n",
    "dataframe_path = main_data_path + '/session_entries.csv'\n",
    "file = pd.read_csv(dataframe_path)[['path', 'name', 'emotion', 'session']]\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d998f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [08:35<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "audios = []\n",
    "ct = 0\n",
    "for i in tqdm(range(math.floor(len(file['path'])/2))):\n",
    "    path = file['path'][i]\n",
    "    wave, sr = torchaudio.load(path)\n",
    "    if sr != bundle.sample_rate:\n",
    "        wave = torchaudio.functional.resample(wave, sr, bundle.sample_rate)\n",
    "    with torch.inference_mode():\n",
    "        feature, _ = extractor.extract_features(wave)\n",
    "    \n",
    "    feature = [f[0] for f in feature]\n",
    "    audio = torch.stack(feature)\n",
    "    audios.append(audio)\n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed5bd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0602ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                           | 87/302 [02:45<07:41,  2.14s/it]"
     ]
    }
   ],
   "source": [
    "audios2 = []\n",
    "for i in tqdm(range(ct, len(file['path']))):\n",
    "    path = file['path'][i]\n",
    "    wave, sr = torchaudio.load(path)\n",
    "    if sr != bundle.sample_rate:\n",
    "        wave = torchaudio.functional.resample(wave, sr, bundle.sample_rate)\n",
    "    with torch.inference_mode():\n",
    "        feature, _ = extractor.extract_features(wave)\n",
    "    \n",
    "    feature = [f[0] for f in feature]\n",
    "    audio = torch.stack(feature)\n",
    "    audios2.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80bc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e731f6",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a63229",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = 1\n",
    "train = file[file['session'] != holdout]\n",
    "train_audios = [audios[i] for i in range(len(audios)) if file['session'][i] != holdout]\n",
    "test = file[file['session'] == holdout]\n",
    "test_audios = [audios[i] for i in range(len(audios)) if file['session'][i] == holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, audios, labels, label_transform):\n",
    "        super(MyDataSet).__init__()\n",
    "        self.audios = audios\n",
    "        self.labels = labels\n",
    "        self.label_transform = label_transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label_transform[self.labels[idx]]\n",
    "        audio = self.audios[idx]\n",
    "        length = audio.size(1)\n",
    "        return audio, length, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(data):\n",
    "    audios, lengths, labels = zip(*data)\n",
    "    max_len = max(lengths)\n",
    "    n_ftrs = audios[0].size(2)\n",
    "    n_dims = audios[0].size(0)\n",
    "    features = torch.zeros((len(audios), n_dims, max_len, n_ftrs))\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        j, k = audios[i].size(1), audios[i].size(2)\n",
    "        features[i] = torch.cat([audios[i], torch.zeros((n_dims, max_len - j, k))], dim=1)\n",
    "\n",
    "    return features, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eaf7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['neutral', 'angry', 'happy', 'sad', 'fear', 'disgust']\n",
    "cate_dic = {}\n",
    "for i, cate in enumerate(categories):\n",
    "    cate_dic[cate] = i\n",
    "cate_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MyDataSet(train_audios, train['emotion'].tolist(), cate_dic)\n",
    "trainloader_args = dict(batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, **trainloader_args, \n",
    "                              collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataSet(test_audios, test['emotion'].tolist(), cate_dic)\n",
    "testloader_args = dict(batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, **testloader_args, \n",
    "                             collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978123ac",
   "metadata": {},
   "source": [
    "# 3CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ICASSP3CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dims = 12, embed_size=128, hidden_size=512, num_lstm_layers = 2, bidirectional = False, label_size=7):\n",
    "        super().__init__()\n",
    "        self.n_layers = num_lstm_layers \n",
    "        self.hidden = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.aggr = nn.Conv1d(in_channels=dims, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.embed = nn.Linear(in_features = vocab_size, out_features = embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(3 * embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 3 * embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_lstm_layers, \n",
    "                            bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(in_features = 2 * hidden_size if bidirectional else hidden_size, \n",
    "                                out_features = label_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "        n, d, b, t = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        input = self.aggr(x)\n",
    "        input = torch.reshape(input, (n, b, t))\n",
    "        input = self.embed(input)\n",
    "\n",
    "        batch_size = input.size(0)\n",
    "        input = input.transpose(1,2)    # (B,T,H) -> (B,H,T)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_n = hn.view(self.n_layers, 2, batch_size, self.hidden)\n",
    "            h_n = torch.cat([ h_n[-1, 0,:], h_n[-1,1,:] ], dim = 1)\n",
    "        else:\n",
    "            h_n = hn[-1]\n",
    "\n",
    "        logits = self.linear(h_n)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf260032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "model = ICASSP3CNN(768)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    model.train()\n",
    "    for batch, (x, length, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, length)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()\n",
    "\n",
    "        #model outputs\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    train_loss = train_loss/batch_cnt\n",
    "    train_accuracy = acc_cnt/(acc_cnt+err_cnt)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f\"epoch:{epoch+1}, train accu:{train_accuracy:.4f},\", f\"train loss:{train_loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
