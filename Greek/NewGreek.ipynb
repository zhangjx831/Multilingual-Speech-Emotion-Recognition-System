{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafae421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "regex = re.compile(r'\\[.+\\]\\n', re.IGNORECASE)\n",
    "file_paths, file_names, emotions, audios = [], [], [], []\n",
    "emotion_map = {'anger': 'angry', 'happiness': 'happy', 'sadness': 'sad', 'fear': 'fear',\n",
    "              'disgust': 'disgust'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5455a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n",
      "0.12.1+cu113\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dd4e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "extractor = bundle.get_model()\n",
    "print(extractor.__class__)\n",
    "print(bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16271443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'Tools and Documentation', 'disgust', 'sadness', 'happiness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('emotiondata/emotion_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a26601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                             | 2/5 [01:49<02:47, 55.79s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "folder_list = ['anger', 'disgust', 'fear', 'happiness', 'sadness']\n",
    "audios = []\n",
    "labels = []\n",
    "for folder in tqdm(folder_list):\n",
    "    cur_file_list = os.listdir(f'emotiondata/emotion_data/{folder}')\n",
    "    for i in cur_file_list:\n",
    "        if 'wav' not in i:\n",
    "            continue\n",
    "        file_path = f'emotiondata/emotion_data/{folder}/{i}'\n",
    "        \n",
    "#         wave, sr = torchaudio.load(file_path)\n",
    "        try:\n",
    "            wave, sr = torchaudio.load(file_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if sr != bundle.sample_rate:\n",
    "            wave = torchaudio.functional.resample(wave, sr, bundle.sample_rate)\n",
    "        with torch.inference_mode():\n",
    "            feature, _ = extractor.extract_features(wave)\n",
    "        feature = [f[0] for f in feature]\n",
    "        audio = torch.stack(feature)\n",
    "        audios.append(audio)\n",
    "        \n",
    "        \n",
    "        file_paths.append(file_path)\n",
    "        file_names.append(i)\n",
    "        emotion = emotion_map[folder]\n",
    "        emotions.append(emotion)\n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             wave, sr = torchaudio.load(file_path)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#         wave = wave.to(device)\n",
    "#         labels.append(folder)\n",
    "#         if sr != bundle.sample_rate:\n",
    "#             wave = torchaudio.functional.resample(wave, sr, bundle.sample_rate)\n",
    "#         with torch.inference_mode():\n",
    "#             features, _ = model.extract_features(wave)\n",
    "# #         audios.append(features[layer][0])\n",
    "#         audios.append(torch.mean(torch.stack(features), dim=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.DataFrame({'path':file_paths, 'name': file_names, 'emotion': emotions, 'audio': audios})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27879e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_path = './wav2vecbase_mean.csv'\n",
    "file.to_csv(dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "spaths, semotions = shuffle(file_paths, emotions, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(spaths, semotions, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f496ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, audios, labels, label_transform):\n",
    "        super(MyDataSet).__init__()\n",
    "        self.audios = audios\n",
    "        self.labels = labels\n",
    "        self.label_transform = label_transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label_transform[self.labels[idx]]\n",
    "        audio = self.audios[idx]\n",
    "        length = audio.size(1)\n",
    "        return audio, length, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(data):\n",
    "    audios, lengths, labels = zip(*data)\n",
    "    max_len = max(lengths)\n",
    "    n_ftrs = audios[0].size(2)\n",
    "    n_dims = audios[0].size(0)\n",
    "    features = torch.zeros((len(audios), n_dims, max_len, n_ftrs))\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        j, k = audios[i].size(1), audios[i].size(2)\n",
    "        features[i] = torch.cat([audios[i], torch.zeros((n_dims, max_len - j, k))], dim=1)\n",
    "\n",
    "    return features, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['angry', 'happy', 'sad', 'fear', 'disgust']\n",
    "cate_dic = {}\n",
    "for i, cate in enumerate(categories):\n",
    "    cate_dic[cate] = i\n",
    "cate_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MyDataSet(X_train, y_train, cate_dic)\n",
    "trainloader_args = dict(batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, **trainloader_args, \n",
    "                              collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataSet(X_val, y_val, cate_dic)\n",
    "testloader_args = dict(batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, **testloader_args, \n",
    "                             collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30207c68",
   "metadata": {},
   "source": [
    "### 3CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ICASSP3CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dims = 12, embed_size=128, hidden_size=512, num_lstm_layers = 2, bidirectional = False, label_size=7):\n",
    "        super().__init__()\n",
    "        self.n_layers = num_lstm_layers \n",
    "        self.hidden = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.aggr = nn.Conv1d(in_channels=dims, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.embed = nn.Linear(in_features = vocab_size, out_features = embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(3 * embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 3 * embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_lstm_layers, \n",
    "                            bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(in_features = 2 * hidden_size if bidirectional else hidden_size, \n",
    "                                out_features = label_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "        n, d, b, t = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        input = self.aggr(x)\n",
    "        input = torch.reshape(input, (n, b, t))\n",
    "        input = self.embed(input)\n",
    "\n",
    "        batch_size = input.size(0)\n",
    "        input = input.transpose(1,2)    # (B,T,H) -> (B,H,T)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_n = hn.view(self.n_layers, 2, batch_size, self.hidden)\n",
    "            h_n = torch.cat([ h_n[-1, 0,:], h_n[-1,1,:] ], dim = 1)\n",
    "        else:\n",
    "            h_n = hn[-1]\n",
    "\n",
    "        logits = self.linear(h_n)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24d5df",
   "metadata": {},
   "source": [
    "### Train Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbeb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "model = ICASSP3CNN(768)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    model.train()\n",
    "    for batch, (x, length, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, length)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()\n",
    "\n",
    "        #model outputs\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    train_loss = train_loss/batch_cnt\n",
    "    train_accuracy = acc_cnt/(acc_cnt+err_cnt)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    model.eval()\n",
    "\n",
    "    for x, lengths, y in test_dataloader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        valid_loss += loss.cpu().item()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    valid_loss = valid_loss/batch_cnt\n",
    "    valid_accuracy = acc_cnt/(acc_cnt+err_cnt)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f\"epoch:{epoch+1}, train accu:{train_accuracy:.4f},\", \n",
    "          f\"train loss:{train_loss:.2f}, valid accu:{valid_accuracy:.4f},\", \n",
    "          f\"valid loss:{valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a8981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
