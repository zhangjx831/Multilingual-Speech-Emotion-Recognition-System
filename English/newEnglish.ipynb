{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcff203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "regex = re.compile(r'\\[.+\\]\\n', re.IGNORECASE)\n",
    "file_paths, file_names, emotions = [], [], []\n",
    "emotion_map = {'neu': 'neutral', 'ang': 'angry', 'hap': 'happy', 'exc': 'happy', 'sad': 'sad'}\n",
    "\n",
    "for session in range(1, 6):\n",
    "    emo_evaluation_dir = f'/home/jz3313/IEMOCAP_full_release/Session{session}/dialog/EmoEvaluation/'\n",
    "    file_dir = f'/home/jz3313/IEMOCAP_full_release/Session{session}/sentences/wav/'\n",
    "    evaluation_files = [l for l in os.listdir(emo_evaluation_dir) if 'Ses' in l]\n",
    "    for file in evaluation_files:\n",
    "        with open(emo_evaluation_dir + file) as f:\n",
    "            content = f.read()\n",
    "        lines = re.findall(regex, content)\n",
    "        for line in lines[1:]:  # the first line is a header\n",
    "            start_end_time, wav_file_name, emotion, val_act_dom = line.strip().split('\\t')\n",
    "            dir_name = '_'.join(wav_file_name.split('_')[:-1])\n",
    "            if emotion in emotion_map:\n",
    "                file_paths.append(file_dir+dir_name+'/'+wav_file_name+'.wav')\n",
    "                file_names.append(wav_file_name)\n",
    "                emotions.append(emotion_map[emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cd075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.DataFrame({'path':file_paths, 'name': file_names, 'emotion': emotions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277cf819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>name</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jz3313/IEMOCAP_full_release/Session1/sen...</td>\n",
       "      <td>Ses01F_impro02_F000</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jz3313/IEMOCAP_full_release/Session1/sen...</td>\n",
       "      <td>Ses01F_impro02_F001</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jz3313/IEMOCAP_full_release/Session1/sen...</td>\n",
       "      <td>Ses01F_impro02_F002</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jz3313/IEMOCAP_full_release/Session1/sen...</td>\n",
       "      <td>Ses01F_impro02_F003</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jz3313/IEMOCAP_full_release/Session1/sen...</td>\n",
       "      <td>Ses01F_impro02_F004</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path                 name  \\\n",
       "0  /home/jz3313/IEMOCAP_full_release/Session1/sen...  Ses01F_impro02_F000   \n",
       "1  /home/jz3313/IEMOCAP_full_release/Session1/sen...  Ses01F_impro02_F001   \n",
       "2  /home/jz3313/IEMOCAP_full_release/Session1/sen...  Ses01F_impro02_F002   \n",
       "3  /home/jz3313/IEMOCAP_full_release/Session1/sen...  Ses01F_impro02_F003   \n",
       "4  /home/jz3313/IEMOCAP_full_release/Session1/sen...  Ses01F_impro02_F004   \n",
       "\n",
       "   emotion  \n",
       "0      sad  \n",
       "1      sad  \n",
       "2      sad  \n",
       "3  neutral  \n",
       "4      sad  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4153e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "spaths, semotions = shuffle(file_paths, emotions, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a55670",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(spaths, semotions, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371d6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n",
      "0.12.1+cu113\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4675fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "extractor = bundle.get_model()\n",
    "print(extractor.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a75bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, labels, label_transform):\n",
    "        super(MyDataSet).__init__()\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.label_transform = label_transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.label_transform[self.labels[idx]]\n",
    "        wave, sr = torchaudio.load(path)\n",
    "        if sr != bundle.sample_rate:\n",
    "            wave = torchaudio.functional.resample(wave, sr, bundle.sample_rate)\n",
    "        with torch.inference_mode():\n",
    "            feature, _ = extractor.extract_features(wave)\n",
    "        feature = [f[0] for f in feature]\n",
    "        audio = torch.stack(feature)\n",
    "        length = audio.size(1)\n",
    "        return audio, length, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141ec40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(data):\n",
    "    audios, lengths, labels = zip(*data)\n",
    "    max_len = max(lengths)\n",
    "    n_ftrs = audios[0].size(2)\n",
    "    n_dims = audios[0].size(0)\n",
    "    features = torch.zeros((len(audios), n_dims, max_len, n_ftrs))\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        j, k = audios[i].size(1), audios[i].size(2)\n",
    "        features[i] = torch.cat([audios[i], torch.zeros((n_dims, max_len - j, k))], dim=1)\n",
    "\n",
    "    return features, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0738ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0, 'happy': 1, 'neutral': 2, 'sad': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['angry', 'happy', 'neutral', 'sad']\n",
    "cate_dic = {}\n",
    "for i, cate in enumerate(categories):\n",
    "    cate_dic[cate] = i\n",
    "cate_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa549a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MyDataSet(X_train, y_train, cate_dic)\n",
    "trainloader_args = dict(batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, **trainloader_args, \n",
    "                              collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataSet(X_val, y_val, cate_dic)\n",
    "testloader_args = dict(batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, **testloader_args, \n",
    "                             collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4848c",
   "metadata": {},
   "source": [
    "## Train with 3CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ef1a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ICASSP3CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dims = 12, embed_size=128, hidden_size=512, num_lstm_layers = 2, bidirectional = False, label_size=7):\n",
    "        super().__init__()\n",
    "        self.n_layers = num_lstm_layers \n",
    "        self.hidden = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.aggr = nn.Conv1d(in_channels=dims, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.embed = nn.Linear(in_features = vocab_size, out_features = embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(3 * embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 3 * embed_size, \n",
    "                            hidden_size = hidden_size, \n",
    "                            num_layers = num_lstm_layers, \n",
    "                            bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(in_features = 2 * hidden_size if bidirectional else hidden_size, \n",
    "                                out_features = label_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "        n, d, b, t = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        input = self.aggr(x)\n",
    "        input = torch.reshape(input, (n, b, t))\n",
    "        input = self.embed(input)\n",
    "\n",
    "        batch_size = input.size(0)\n",
    "        input = input.transpose(1,2)    # (B,T,H) -> (B,H,T)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_n = hn.view(self.n_layers, 2, batch_size, self.hidden)\n",
    "            h_n = torch.cat([ h_n[-1, 0,:], h_n[-1,1,:] ], dim = 1)\n",
    "        else:\n",
    "            h_n = hn[-1]\n",
    "\n",
    "        logits = self.linear(h_n)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7d392",
   "metadata": {},
   "source": [
    "### Model Traning on each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75762bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                               | 1/50 [41:15<33:41:44, 2475.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, train accu:0.3929, train loss:1.25, valid accu:0.4661, valid loss:1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██████▍                                                                                                                                                          | 2/50 [1:22:08<32:49:41, 2462.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, train accu:0.4772, train loss:1.10, valid accu:0.4923, valid loss:1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████████▋                                                                                                                                                       | 3/50 [2:02:54<32:02:58, 2454.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, train accu:0.5362, train loss:1.04, valid accu:0.5122, valid loss:1.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|████████████▉                                                                                                                                                    | 4/50 [2:43:32<31:16:55, 2448.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, train accu:0.5472, train loss:1.01, valid accu:0.5104, valid loss:1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████████████                                                                                                                                                 | 5/50 [3:24:03<30:31:35, 2442.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, train accu:0.5500, train loss:1.00, valid accu:0.4932, valid loss:1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▎                                                                                                                                             | 6/50 [4:05:21<29:59:51, 2454.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, train accu:0.5775, train loss:0.97, valid accu:0.5077, valid loss:1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████████████████▌                                                                                                                                          | 7/50 [4:46:42<29:25:05, 2462.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, train accu:0.5992, train loss:0.94, valid accu:0.5131, valid loss:1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████████████████▊                                                                                                                                       | 8/50 [5:27:59<28:47:08, 2467.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, train accu:0.6311, train loss:0.91, valid accu:0.5691, valid loss:1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████████████▉                                                                                                                                    | 9/50 [6:08:59<28:04:27, 2465.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, train accu:0.6598, train loss:0.85, valid accu:0.5718, valid loss:1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████████████████████                                                                                                                                | 10/50 [6:49:06<27:11:22, 2447.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, train accu:0.6668, train loss:0.85, valid accu:0.5736, valid loss:1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|███████████████████████████████████▏                                                                                                                            | 11/50 [7:29:10<26:22:07, 2434.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11, train accu:0.6926, train loss:0.79, valid accu:0.5971, valid loss:1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████████████████████████▍                                                                                                                         | 12/50 [8:09:16<25:36:04, 2425.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12, train accu:0.7118, train loss:0.75, valid accu:0.6007, valid loss:1.00\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "model = ICASSP3CNN(768)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    model.train()\n",
    "    for batch, (x, length, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, length)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()\n",
    "\n",
    "        #model outputs\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    train_loss = train_loss/batch_cnt\n",
    "    train_accuracy = acc_cnt/(acc_cnt+err_cnt)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    model.eval()\n",
    "\n",
    "    for x, lengths, y in test_dataloader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        valid_loss += loss.cpu().item()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    valid_loss = valid_loss/batch_cnt\n",
    "    valid_accuracy = acc_cnt/(acc_cnt+err_cnt)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f\"epoch:{epoch+1}, train accu:{train_accuracy:.4f},\", \n",
    "          f\"train loss:{train_loss:.2f}, valid accu:{valid_accuracy:.4f},\", \n",
    "          f\"valid loss:{valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'/home/xg2378/models/wav2vecbase_{layer}.pth'\n",
    "\n",
    "torch.save({'epoch':epochs,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict()},\n",
    "            model_path)\n",
    "\n",
    "metadata = pd.DataFrame({'epoch':range(epochs), 'train loss':train_losses, \n",
    "                         'valid loss':valid_losses, 'train accu':train_accuracies, \n",
    "                         'valid_accu':valid_accuracies})\n",
    "metadata.to_csv(f'/home/xg2378/results/acc_loss/wav2vecbase_{layer}.csv ', \n",
    "                index=False)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epochs), train_losses, label='train')\n",
    "plt.plot(range(epochs), valid_losses, label='valid')\n",
    "plt.legend()\n",
    "plt.title('training and validation loss')\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_accuracies, label='train')\n",
    "plt.plot(range(epochs), valid_accuracies, label='valid')\n",
    "plt.legend()\n",
    "plt.title('training and validation accuracy')\n",
    "plt.show()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for inputs, lengths, labels in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(inputs, lengths) # Feed Network\n",
    "\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ecd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "classes = list(set([v[1] for k,v in waveforms_results_dict.items()]))\n",
    "df_cm = pd.DataFrame(cf, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'English Wav2VecBase Layer {layer} Confusion Matrix')\n",
    "plt.savefig(f'/home/xg2378/results/confusion_matrix/wav2vecbase_{layer}.png')\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
